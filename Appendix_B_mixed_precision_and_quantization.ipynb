{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Appendix B – Mixed Precision and Quantization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code for appendix B._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ageron/handson-mlp/blob/main/Appendix_B_mixed_precision_and_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-mlp/blob/main/Appendix_B_mixed_precision_and_quantization.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires Python 3.10 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we using Colab or Kaggle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And PyTorch ≥ 2.6.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging.version import Version\n",
    "import torch\n",
    "\n",
    "assert Version(torch.__version__) >= Version(\"2.6.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in earlier chapters, let's define the default font sizes to make the figures prettier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter can be very slow without a hardware accelerator, so if we can find one, let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's issue a warning if there's no hardware accelerator available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    print(\"Neural nets can be very slow without a hardware accelerator.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
    "              \"accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Number Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supports several types for floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "fp64 = torch.tensor(1.234e56, dtype=torch.float64)\n",
    "\n",
    "fp32 = torch.tensor(1.234e56, dtype=torch.float32)\n",
    "\n",
    "fp16 = torch.tensor(1.234e56, dtype=torch.float16)\n",
    "bf16 = torch.tensor(1.234e56, dtype=torch.bfloat16)\n",
    "\n",
    "fp8 = torch.tensor(1.234e56, dtype=torch.float8_e5m2)\n",
    "bf8 = torch.tensor(1.234e56, dtype=torch.float8_e4m3fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And several types of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i64 = torch.tensor(123456, dtype=torch.int64)\n",
    "u64 = torch.tensor(123456, dtype=torch.uint64)\n",
    "\n",
    "i32 = torch.tensor(123456, dtype=torch.int32)\n",
    "u32 = torch.tensor(123456, dtype=torch.uint32)\n",
    "\n",
    "i16 = torch.tensor(1234, dtype=torch.int16)\n",
    "u16 = torch.tensor(1234, dtype=torch.uint16)\n",
    "\n",
    "i8 = torch.tensor(123, dtype=torch.int8)\n",
    "u8 = torch.tensor(123, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch does not yet have direct support for 4-bit integers, but we can pack two 4-bit integers into one 8-bit integer using bitwise operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u4 = torch.tensor((12 << 4) | 5, dtype=torch.uint8)  # stores 12 and 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can unpack the values by reversing the bit operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12, dtype=torch.uint8), tensor(5, dtype=torch.uint8))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u4 >> 4, u4 & 0xF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit more involved for signed integers, because they are represented using 2's complement, so it's best to store them using an unsigned byte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi, lo = -5, -8\n",
    "i4 = torch.tensor(((hi & 0xF) << 4) | (lo & 0xF), dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpacking is also a bit trickier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5, -8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi, lo = ((i4 >> 4).to(torch.int8) ^ 8) - 8, ((i4 & 0xF).to(torch.int8) ^ 8) - 8\n",
    "hi.item(), lo.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar approach can be used to store four 2-bit integers in an unsigned byte (this is left as an exercise to the reader)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, here's an example showing how ternary values can be represented efficiently, packing 5 values per byte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_ternary(values):\n",
    "    factors = torch.tensor([1, 3, 9, 27, 81], dtype=torch.uint8)\n",
    "    return ((values + 1).to(torch.uint8) * factors).sum(dim=-1).to(torch.uint8)\n",
    "\n",
    "def unpack_ternary(packed):\n",
    "    vals = torch.empty(packed.shape + (5,), dtype=torch.int8)\n",
    "    for i in range(5):\n",
    "        vals[..., i] = packed % 3\n",
    "        packed //= 3\n",
    "    return vals - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66, dtype=torch.uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.tensor([-1, 0, 0, 1, -1], dtype=torch.int8)\n",
    "packed = pack_ternary(values)\n",
    "packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -1, -1, -1, -1], dtype=torch.int8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpack_ternary(torch.tensor(0, dtype=torch.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced Precision Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1))\n",
    "# [...] pretend the 32-bit model is trained here\n",
    "model.half()  # convert the model parameters to half precision (16 bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(3, 10, dtype=torch.float16)  # some 16-bit input\n",
    "y_pred = model(X)  # 16-bit output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(10, 100, dtype=torch.float16), nn.ReLU(),\n",
    "                      nn.Linear(100, 1, dtype=torch.float16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPT is not fully supported yet on MPS devices\n",
    "device2 = \"cpu\" if device == \"mps\" else device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler\n",
    "\n",
    "def train_mpt(model, optimizer, criterion, train_loader, n_epochs,\n",
    "              dtype=torch.float16, init_scale=2.0**16):\n",
    "    grad_scaler = GradScaler(device=device2, init_scale=init_scale)\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device2), y_batch.to(device2)\n",
    "            with torch.autocast(device_type=device2, dtype=dtype):\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            print(f\"\\rEpoch: {epoch + 1}, loss: {total_loss:.3f}\", end=\"\")\n",
    "            grad_scaler.scale(loss).backward()\n",
    "            grad_scaler.step(optimizer)\n",
    "            grad_scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 32.218\n",
      "Epoch: 2, loss: 30.540\n",
      "Epoch: 3, loss: 29.854\n",
      "Epoch: 4, loss: 29.382\n",
      "Epoch: 5, loss: 29.013\n",
      "Epoch: 6, loss: 28.641\n",
      "Epoch: 7, loss: 28.321\n",
      "Epoch: 8, loss: 28.004\n",
      "Epoch: 9, loss: 27.736\n",
      "Epoch: 10, loss: 27.444\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1)).to(device2)\n",
    "\n",
    "X_train = torch.randn(1000, 10)\n",
    "y_train = torch.randn(1000, 1)\n",
    "train_set = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_set, batch_size=32)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.01)\n",
    "mse = torch.nn.MSELoss()\n",
    "train_mpt(model, optimizer, mse, train_loader, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymmetric linear quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0988, -0.0988,  0.6012,  0.0000], size=(4,), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.002745098201557994,\n",
       "       zero_point=36)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([0.1, -0.1, 0.6, 0.0])  # 32-bit floats\n",
    "s = (w.max() - w.min()) / 255.  # compute the scale\n",
    "z = -(w.min() / s).round()  # compute the zero point\n",
    "qw = torch.quantize_per_tensor(w, scale=s, zero_point=z, dtype=torch.quint8)\n",
    "qw  # this is a quantized tensor internally represented using integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0988, -0.0988,  0.6012,  0.0000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qw.dequantize()  # back to 32-bit floats (close to the original tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric linear quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, -0.9400,  0.9178,  0.9326], size=(4,), dtype=torch.qint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.007401574868708849,\n",
       "       zero_point=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([0.0, -0.94, 0.92, 0.93])  # 32-bit floats\n",
    "s = w.abs().max() / 127.\n",
    "qw = torch.quantize_per_tensor(w, scale=s, zero_point=0, dtype=torch.qint8)\n",
    "qw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "machine = platform.machine().lower()\n",
    "engine = \"qnnpack\" if (\"arm\" in machine or \"aarch64\" in machine) else \"x86\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import quantize_dynamic\n",
    "\n",
    "model = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1))\n",
    "# [...] pretend the 32-bit model is trained here\n",
    "torch.backends.quantized.engine = engine\n",
    "qmodel = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "X = torch.randn(3, 10)\n",
    "y_pred = qmodel(X)  # float inputs and outputs, but quantized internally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_loader = train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Quantize(scale=tensor([0.0604]), zero_point=tensor([57]), dtype=torch.quint8)\n",
       "  (1): QuantizedLinear(in_features=10, out_features=100, scale=0.040152497589588165, zero_point=65, qscheme=torch.per_channel_affine)\n",
       "  (2): ReLU()\n",
       "  (3): QuantizedLinear(in_features=100, out_features=1, scale=0.01066256407648325, zero_point=68, qscheme=torch.per_channel_affine)\n",
       "  (4): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torch.ao.quantization import get_default_qconfig, QuantStub, DeQuantStub\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(QuantStub(),\n",
    "                      nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1),\n",
    "                      DeQuantStub())\n",
    "# [...] pretend the 32-bit model is trained here\n",
    "model.qconfig = get_default_qconfig(engine)\n",
    "torch.ao.quantization.prepare(model, inplace=True)\n",
    "for X_batch, _ in calibration_loader:\n",
    "    model(X_batch)\n",
    "torch.ao.quantization.convert(model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_default_qconfig(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([100, 10]) torch.qint8\n",
      "3 torch.Size([1, 100]) torch.qint8\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, 'weight'):\n",
    "        print(name, module.weight().shape, module.weight().dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2133],\n",
       "        [ 0.0000],\n",
       "        [-0.0320]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "X_batch = torch.randn(3, 10)\n",
    "y_pred = model(X_batch)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some modules can be fused, such as fusing `Linear` and `ReLU` into a `LinearReLU` layer. For this, we can use the `fuse_modules()` function, and give it a list of modules to fuse. Since we reference these modules by name, we must name each module that we want to fuse. This is usually done by creating a custom module with one attribute for each module, but another option is to use an `OrderedDict` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (quantize): QuantStub()\n",
       "  (linear1): LinearReLU(\n",
       "    (0): Linear(in_features=10, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (relu1): Identity()\n",
       "  (linear2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (dequantize): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    (\"quantize\", QuantStub()),\n",
    "    (\"linear1\", nn.Linear(10, 100)),\n",
    "    (\"relu1\", nn.ReLU()),\n",
    "    (\"linear2\", nn.Linear(100, 1)),\n",
    "    (\"dequantize\", DeQuantStub())]))\n",
    "# [...] pretend the 32-bit model is trained here\n",
    "torch.quantization.fuse_modules(model, [['linear1', 'relu1']], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we quantize the model, we get a `QuantizedLinearReLU` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (quantize): Quantize(scale=tensor([0.0604]), zero_point=tensor([57]), dtype=torch.quint8)\n",
       "  (linear1): QuantizedLinearReLU(in_features=10, out_features=100, scale=0.019709425047039986, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "  (relu1): Identity()\n",
       "  (linear2): QuantizedLinear(in_features=100, out_features=1, scale=0.01066256407648325, zero_point=68, qscheme=torch.per_channel_affine)\n",
       "  (dequantize): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.qconfig = get_default_qconfig(engine)\n",
    "torch.ao.quantization.prepare(model, inplace=True)\n",
    "for X_batch, _ in calibration_loader:\n",
    "    model(X_batch)\n",
    "torch.ao.quantization.convert(model, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can use ExecuTorch or TFLite or any other solution to deploy your model to the target device. For example, let's export this fused and quantized module to ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    %pip install -qU onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    X_batch,\n",
    "    \"my_quantized_model.onnx\",\n",
    "    opset_version=13,\n",
    "    input_names=['float_input'],\n",
    "    output_names=['float_output']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization-Aware Training (QAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1153\n",
      "Epoch 1, Loss: 1.1081\n",
      "Epoch 2, Loss: 1.1034\n",
      "Epoch 3, Loss: 1.0954\n",
      "Epoch 4, Loss: 1.0909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Quantize(scale=tensor([0.0473]), zero_point=tensor([60]), dtype=torch.quint8)\n",
       "  (1): QuantizedLinear(in_features=10, out_features=100, scale=0.03720283508300781, zero_point=62, qscheme=torch.per_channel_affine)\n",
       "  (2): ReLU()\n",
       "  (3): QuantizedLinear(in_features=100, out_features=1, scale=0.008233007043600082, zero_point=68, qscheme=torch.per_channel_affine)\n",
       "  (4): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.ao.quantization import get_default_qat_qconfig\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(QuantStub(),\n",
    "                      nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1),\n",
    "                      DeQuantStub())\n",
    "model.qconfig = get_default_qat_qconfig(engine)\n",
    "torch.ao.quantization.prepare_qat(model, inplace=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "X_train = torch.randn(128, 10)\n",
    "y_train = torch.randn(128, 1)\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "torch.ao.quantization.convert(model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1811],\n",
       "        [ 0.0412],\n",
       "        [-0.0082]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "X_batch = torch.randn(3, 10)\n",
    "y_pred = model(X_batch)  # float inputs & outputs, but quantized internally\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face BitsAndBytes (bnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB or IS_KAGGLE:\n",
    "    %pip install -qU bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f54c52fbc984eb1be2558aad7eff1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc10ab64f28d45ce9c7ea73baad928dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167fa9f8512f410889f91c30317f738b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "  from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "  model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "  bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "                                  bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",\n",
    "                                               quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8af0b33325b486bb58a930809cb2997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb899c5ce3f64632b92ff76063197b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ce604b79ec4d6ab4942781a575557e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8d2603b59544fc8ba20e8be4e2c5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow! I love this! I'm going to try\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "  from transformers import AutoTokenizer\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  input_ids = tokenizer(\"Wow!\", return_tensors=\"pt\").input_ids.to(model.device)\n",
    "  output_ids = model.generate(input_ids, max_new_tokens=10)\n",
    "  print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "  from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "  bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "                                  bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                  bnb_4bit_use_double_quant=True)\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",\n",
    "                                               quantization_config=bnb_config)\n",
    "  model = prepare_model_for_kbit_training(model)\n",
    "  lora_config = LoraConfig(r=16, lora_alpha=32,\n",
    "                           target_modules=[\"q_proj\", \"v_proj\"],\n",
    "                           lora_dropout=0.05, bias=\"none\",\n",
    "                           task_type=\"CAUSAL_LM\")\n",
    "  peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/96.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "if IS_COLAB or IS_KAGGLE:\n",
    "    %pip install -qU gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df04ba1c373c48ec994804e155616d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/33.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838bdd2f42a2402380777b65a4207e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting and de-quantizing GGUF tensors...:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n",
    "filename = \"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"\n",
    "\n",
    "torch_dtype = torch.float32 # could be torch.float16 or torch.bfloat16 too\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename,\n",
    "                                             dtype=torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
