{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Appendix D – Relative Positional Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code in appendix D._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ageron/handson-mlp/blob/main/Appendix_D_relative_positional_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-mlp/blob/main/Appendix_D_relative_positional_encoding.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires Python 3.10 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And PyTorch ≥ 2.6.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging.version import Version\n",
    "import torch\n",
    "\n",
    "assert Version(torch.__version__) >= Version(\"2.6.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias RPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttentionWithBiasRPE(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1, r_max=128):\n",
    "        super().__init__()\n",
    "        self.h = num_heads\n",
    "        self.d = embed_dim // num_heads\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        ###### ADDED\n",
    "        self.biases = nn.Parameter(torch.zeros(num_heads, 2 * r_max - 1))\n",
    "        ######\n",
    "\n",
    "    ###### ADDED\n",
    "    def gather_biases(self, Lq, Lk):\n",
    "        h, n_biases = self.biases.shape  # [h, 2 * r_max - 1]\n",
    "        r_max = (n_biases + 1) // 2\n",
    "        pos_q = torch.arange(Lq, device=self.biases.device)  # [0, ..., Lq - 1]\n",
    "        pos_k = torch.arange(Lk, device=self.biases.device)  # [0, ..., Lk - 1]\n",
    "        rel_pos = pos_q[:, None] - pos_k[None, :]  # [Lq, Lk] (contains i - j)\n",
    "        rel_pos = rel_pos.clamp(-r_max + 1, r_max - 1) + r_max - 1  # [Lq, Lk]\n",
    "        return self.biases[:, rel_pos]  # [h, Lq, Lk]\n",
    "    ######\n",
    "\n",
    "    def split_heads(self, X):\n",
    "        return X.view(X.size(0), X.size(1), self.h, self.d).transpose(1, 2)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        q = self.split_heads(self.q_proj(query))  # (B, h, Lq, d)\n",
    "        k = self.split_heads(self.k_proj(key))  # (B, h, Lk, d)\n",
    "        v = self.split_heads(self.v_proj(value))  # (B, h, Lv, d) with Lv=Lk\n",
    "        scores = q @ k.transpose(2, 3) / self.d**0.5  # (B, h, Lq, Lk)\n",
    "\n",
    "        ###### ADDED\n",
    "        b = self.gather_biases(query.size(-2), key.size(-2))\n",
    "        scores = scores + b\n",
    "        ######\n",
    "\n",
    "        weights = self.dropout(scores.softmax(dim=-1))  # (B, h, Lq, Lk)\n",
    "        Z = weights @ v  # (B, h, Lq, d)\n",
    "        Z = Z.transpose(1, 2)  # (B, Lq, h, d)\n",
    "        Z = Z.reshape(Z.size(0), Z.size(1), self.h * self.d)  # (B, Lq, h × d)\n",
    "        return (self.out_proj(Z), weights)  # (B, Lq, h × d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 700, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32\n",
    "d_model = 512\n",
    "Lq = 700\n",
    "Lk = Lv = 800\n",
    "\n",
    "Q = torch.randn(batch_size, Lq, d_model)\n",
    "K = torch.randn(batch_size, Lk, d_model)\n",
    "V = torch.randn(batch_size, Lv, d_model)\n",
    "\n",
    "mha = MultiheadAttentionWithBiasRPE(embed_dim=d_model, num_heads=8, r_max=512)\n",
    "output, weights = mha(Q, K, V)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function pre-computes the sines and cosines for RoPE. It returns a tuple containing sine and cosine tables, each of shape `[max_len, d]`. This way, we can easily get the cosine of the angle for the _k_^th^ component of the token at a given position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_cos_sin(d, max_len, base=10_000):\n",
    "    theta = base ** (-torch.arange(0, d, 2).float() / d)  # θₖ, shape: [d // 2]\n",
    "    positions = torch.arange(max_len)  # p, shape: [max_len]\n",
    "    freqs = torch.outer(positions, theta)  # p * θₖ, shape: [max_len, d // 2]\n",
    "    freqs_twice = freqs.repeat_interleave(2, dim=-1)  # shape: [max_len, d]\n",
    "    return freqs_twice.cos(), freqs_twice.sin()  # shape: both [max_len, d]\n",
    "\n",
    "d, max_len = 64, 4_096\n",
    "cos_theta, sin_theta = precompute_rope_cos_sin(d, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a 2D vector (x, y) and rotate it by an angle _θ_, the result is (_x_ cos(_θ_) – _y_ sin(_θ_), _x_ sin(_θ_) + _y_ cos(_θ_)).\n",
    "\n",
    "Let's see how we can apply the multiple 2D rotations required by RoPE in one shot. The token `t` is interpreted as a sequence of 2D coordinates for each subspace:\n",
    "\n",
    "```\n",
    "t = [x0, y0, x1, y1, x2, y2, ...]\n",
    "```\n",
    "\n",
    "First, we will swap each pair of coordinates and multiple the resulting horizontal coordinates by –1:\n",
    "\n",
    "```\n",
    "t' = [-y0, x0, -y1, x1, -y2, x2, ...]\n",
    "```\n",
    "\n",
    "Then, using the cosines and sines we precomputed for all rotation angles (depending on the subspace and the position, we can then compute RoPE like this:\n",
    "\n",
    "```\n",
    "t_rope = t * precomputed_cos + t' * precomputed_sin\n",
    "```\n",
    "\n",
    "Indeed, this will give us:\n",
    "\n",
    "```\n",
    "t_rope = [x0 * cos(θ0) - y0 * sin(θ0), y0 * cos(θ0) + x0 * sin(θ0),\n",
    "          x1 * cos(θ1) - y1 * sin(θ1), y1 * cos(θ1) + x1 * sin(θ1), ...]\n",
    "```\n",
    "\n",
    "That's the result we want: each 2D subspace is rotated by the desired angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rope_rotation(t, cos_theta, sin_theta):\n",
    "    t_grouped = t.reshape(*t.shape[:-1], -1, 2)  # group pairs of dims\n",
    "    t_swapped = t_grouped[..., [1, 0]]  # swap 2D axes: (x, y) -> (y, x)\n",
    "    t_swapped[..., 0] *= -1  # for each pair, (y, x) -> (–y, x)\n",
    "    t_rotated_half = t_swapped.flatten(start_dim=-2)  # [-y0, x0, -y1, x1,...]\n",
    "    L = t.size(-2)\n",
    "    return t * cos_theta[:L] + t_rotated_half * sin_theta[:L]  # same shape as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size, n_heads, Lq, Lk, d = 32, 8, 800, 800, 64\n",
    "Q = torch.randn(batch_size, n_heads, Lq, d)\n",
    "K = torch.randn(batch_size, n_heads, Lk, d)\n",
    "Q_rope = rope_rotation(Q, cos_theta, sin_theta)\n",
    "K_rope = rope_rotation(K, cos_theta, sin_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers.models.llama.modeling_llama as mll\n",
    "\n",
    "config = mll.LlamaConfig(hidden_size = n_heads * d, num_attention_heads=n_heads,\n",
    "                         max_position_embeddings=max_len)\n",
    "rotary_emb = mll.LlamaRotaryEmbedding(config)\n",
    "position_ids = torch.arange(Lq, device=Q.device).unsqueeze(0)\n",
    "cos_theta, sin_theta = rotary_emb(Q, position_ids)  # Q needed for device/dtype\n",
    "Q_rope, K_rope = mll.apply_rotary_pos_emb(Q, K, cos_theta, sin_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALiBi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates the ALiBi bias matrix for a given number of heads and sequence length. The output tensor has a shape of `[num_heads, seq_len, seq_len]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alibi_biases(n_heads, seq_len):\n",
    "    head = torch.arange(1, n_heads + 1, dtype=torch.float32)  # shape: [n_heads]\n",
    "    slopes = torch.pow(2, -8 * head / n_heads)  # [n_heads]\n",
    "    pos = torch.arange(seq_len)  # [seq_len]\n",
    "    distance_matrix = -(pos[:, None] - pos[None, :]).abs()  # [seq_len, seq_len]\n",
    "    return slopes.view(-1, 1, 1) * distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.5000, -1.0000, -1.5000],\n",
       "        [-0.5000,  0.0000, -0.5000, -1.0000],\n",
       "        [-1.0000, -0.5000,  0.0000, -0.5000],\n",
       "        [-1.5000, -1.0000, -0.5000,  0.0000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases = get_alibi_biases(n_heads=8, seq_len=4)\n",
    "biases[0]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
